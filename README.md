# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data about possible bank customers such as job, education, and whether they own a home or not.  We seek to predict whether they will respond to marketing to open a new account.  There is data about which customers responded in the past with the 'y' column

The best performing model was determined by AutoML as the VotingEnsemble with an accuracy of 91.77%

## Scikit-learn Pipeline
The Scikit-learn pipeline uses a custom Python script utilizing the Scikit-learn LogisticRegression model.  This model uses a linear model, and doesn't actually involve regression.  Probabilities are determined by a logistic function, and a regularization is applied.

The data must be cleaned and split into training and test data so that the accuracy may be determined.  The training data is used to train the model, while the test data is used to validate at the end.

There are two parameters to tune.  The first is C, which determines the maximum model complexity.  Higher number may overfit the data, but lower numbers could keep the model too simple.  The second is the maximum number of iterations to find a good fit.  This second value seemed to be the one that could most affect the outcome.  If it was less than 50 the accuracy was lowered.  If the C parameter was set to 0.01 accuracy could be increased slightly.

**What are the benefits of the parameter sampler you chose?**
Uniform may be used for C, which is a floating point number, while the number of iterations must be an integer, so I used choice to limit it down to valid numbers.  I then used the RandomParameterSampling as the simplest option to try out random configurations within the valid number space as I had no idea how the values would affect the numbers.  After the initial sampling a GridSampler could then be used to fine tune if needed.  The numbers seemed fairly consistent for most values, so I did not pursue that option.

**What are the benefits of the early stopping policy you chose?**
The Bandit policy is based on a slack factor and interval.  It will terminate the run if the metric falls outside the specified slack factor from the best run.  Each run only outputs the accuracy once, at the end, and so the policy is never actually applied.  Each run is also fairly quick, usually taking less than 1 minute each.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML determined that VotingEnsemble would give the best results, which applies an ensemble of other models weighted together.  the optimal parameters found are listed below:
"ensembled_iterations": "[0, 1, 13, 3, 9, 11, 19]",
"ensembled_algorithms": "['LightGBM', 'XGBoostClassifier', 'SGD', 'SGD', 'SGD', 'SGD', 'ExtremeRandomTrees']",
"ensemble_weights": "[0.38461538461538464, 0.23076923076923078, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693, 0.07692307692307693]"

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
AutoML generated a better model, but did take a lot more compute time to run through all of the options in the cluster.  It was a lot more automated than the custom Python script which had a set model with a couple of parameters to optimize.  The custom script could definitely take more user time to initially design as a model must be chosen, which may not be able to give optimal results.  If this is the case a new model and new run would need to be coded and applied.  It took more time to come up with the options to pass into the custom Python script than to set up the AutoML run.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Larger data samples could be used, ideally over 100k records.  Breaking up the data into smaller pieces and returning metrics at intervals would allow the early stopping policy to actually take effect.  Using grid sampling could be better than Random for the SciKit-Learn model. and allow it to spend additional time to find results with more combinations.  Allowing AutoML to go beyond the 30 minute time limit would allow it to further investigate better parameters and models.

## Proof of cluster clean up
cpu_cluster.delete() was run, and an image is included
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
